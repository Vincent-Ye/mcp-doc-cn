---
title: "采样"
description: "让您的服务器通过客户端请求LLM补全"
---

采样是一个强大的MCP功能，允许服务器通过客户端向LLM（大型语言模型）请求补全，从而实现复杂的智能行为，同时保持安全性和隐私性。

<Info>
  MCP的这一功能目前尚未在Claude桌面客户端中支持。
</Info>

## 采样如何工作

采样流程包括以下步骤：

1. 服务器向客户端发送一个`sampling/createMessage`请求。
2. 客户端审核请求并可以对此进行修改。
3. 客户端从LLM中采样。
4. 客户端审核生成结果。
5. 客户端将结果返回给服务器。

这种“用户参与”的设计可以确保用户能够控制LLM接收和生成的内容。

## 消息格式

采样请求采用标准化的消息格式：

```typescript
{
  messages: [
    {
      role: "user" | "assistant",
      content: {
        type: "text" | "image",

        // 对于文本：
        text?: string,

        // 对于图片：
        data?: string,             // Base64编码
        mimeType?: string
      }
    }
  ],
  modelPreferences?: {
    hints?: [{
      name?: string                // 建议的模型名称或类别
    }],
    costPriority?: number,         // 0-1，降低成本的重要性
    speedPriority?: number,        // 0-1，低延迟需求的重要性
    intelligencePriority?: number  // 0-1，模型能力需求的重要性
  },
  systemPrompt?: string,
  includeContext?: "none" | "thisServer" | "allServers",
  temperature?: number,
  maxTokens: number,
  stopSequences?: string[],
  metadata?: Record<string, unknown>
}
```

## 请求参数

### 消息（Messages）

`messages`数组包含需要发送给LLM的对话历史。每条消息具备以下内容：

- `role`：可以是"用户"（user）或"助手"（assistant）。
- `content`：消息内容，可以是：
  - 携带`text`字段的文本内容
  - 包含Base64编码的`data`和`mimeType`字段的图片内容

### 模型偏好（Model preferences）

`modelPreferences`对象允许服务器指定模型选择的偏好：

- `hints`：一个模型名称建议数组，客户端可根据建议选择适当的模型：
  - `name`：一个字符串，可与完整或部分模型名称匹配（例如："claude-3"、"sonnet"）。
  - 客户端可以将提示与不同提供商的等效模型进行映射。
  - 多个提示按照优先顺序进行评估。

- 优先级值（归一化为0-1）：
  - `costPriority`：降低成本的重要性。
  - `speedPriority`：降低响应延迟的重要性。
  - `intelligencePriority`：提升高阶能力的重要性。

客户端根据这些偏好以及其可用模型做出最终的模型选择。

### 系统提示（System prompt）

`systemPrompt`字段是可选的，允许服务器请求特定的系统提示。客户端可能修改或忽略此提示。

### 上下文包含（Context inclusion）

`includeContext`参数指定需要包含的MCP上下文范围：

- `"none"`：不包含任何额外上下文。
- `"thisServer"`：包含来自请求服务器的上下文。
- `"allServers"`：包含来自所有连接MCP服务器的上下文。

客户端控制实际包含的上下文内容。

### 采样参数（Sampling parameters）

用以下参数微调LLM采样：

- `temperature`：控制生成随机性（范围0.0到1.0）。
- `maxTokens`：生成的最大令牌数量。
- `stopSequences`：停止生成的序列数组。
- `metadata`：额外的、与提供商相关的特定参数。

## 响应格式

客户端返回一个补全结果：

```typescript
{
  model: string,  // 使用的模型名称
  stopReason?: "endTurn" | "stopSequence" | "maxTokens" | string,
  role: "user" | "assistant",
  content: {
    type: "text" | "image",
    text?: string,
    data?: string,
    mimeType?: string
  }
}
```

## 示例请求

以下是把文本翻译成中文的内容：

---

这是一个从客户端请求采样的示例：
```json
{
  "method": "sampling/createMessage",
  "params": {
    "messages": [
      {
        "role": "user",
        "content": {
          "type": "text",
          "text": "当前目录下有哪些文件？"
        }
      }
    ],
    "systemPrompt": "你是一个有用的文件系统助手。",
    "includeContext": "thisServer",
    "maxTokens": 100
  }
}
```

## 最佳实践

在实现采样时：

1. 始终提供清晰、结构良好的提示
2. 恰当处理文本和图像内容
3. 设置合理的令牌限制
4. 通过`includeContext`包含相关上下文
5. 在使用响应之前进行验证
6. 优雅地处理错误
7. 考虑对采样请求进行速率限制
8. 记录预期的采样行为
9. 使用各种模型参数进行测试
10. 监控采样成本

## 人为监管控制

采样设计时考虑到了人为监督：

### 对于提示

- 客户端应向用户显示建议的提示内容
- 用户应能够修改或拒绝提示
- 系统提示可以被过滤或修改
- 上下文包含由客户端控制

### 对于完成内容

- 客户端应向用户显示生成结果
- 用户应能够修改或拒绝生成内容
- 客户端可以过滤或修改生成内容
- 用户可以控制使用哪种模型

## 安全性考虑

在实现采样时：

- 验证所有消息内容
- 清理敏感信息
- 实现适当的速率限制
- 监控采样使用情况
- 在传输过程中加密数据
- 处理用户数据隐私
- 审计采样请求
- 控制成本暴露
- 实现超时机制
- 优雅地处理模型错误

## 常见模式

### 智能代理工作流

采样支持如下智能代理模式：

- 读取和分析资源
- 基于上下文进行决策
- 生成结构化数据
- 处理多步骤任务
- 提供交互式协助

### 上下文管理

上下文管理的最佳实践：

- 请求最小必要的上下文
- 清晰地组织上下文
- 处理上下文的大小限制
- 根据需要更新上下文
- 清除过时的上下文

### 错误处理

健壮的错误处理应包括：

- 捕获采样失败
- 处理超时错误
- 管理速率限制
- 验证响应
- 提供备用行为
- 适当地记录错误

## 限制

需要注意以下限制：

- 采样依赖于客户端的能力
- 用户控制采样行为
- 上下文大小有一定限制
- 可能存在速率限制
- 需考虑成本因素
- 模型的可用性可能有所不同
- 响应时间可能会有所变化
- 并非支持所有内容类型